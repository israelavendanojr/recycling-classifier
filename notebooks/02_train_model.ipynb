{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6333ed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21dee7b",
   "metadata": {},
   "source": [
    "## Define Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf036f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 224\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    # Resizes images to 224x224, consider using CenterCrop or pad is aspect ratio wanted to be preserved\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    # Randomly flips the image horizontally for data augmentation, improves generalization\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # Converts image to PyTorch tensor\n",
    "    transforms.ToTensor(),\n",
    "    # Normalizes RGB\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])  # Imagenet mean/std\n",
    "])\n",
    "\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
