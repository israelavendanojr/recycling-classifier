{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c9e0203",
   "metadata": {},
   "source": [
    "### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6333ed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18336bd3",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21dee7b",
   "metadata": {},
   "source": [
    "### Define Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf036f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 224\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    # Resizes images to 224x224, consider using CenterCrop or pad is aspect ratio wanted to be preserved\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    # Randomly flips the image horizontally for data augmentation, improves generalization\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # Converts image to PyTorch tensor\n",
    "    transforms.ToTensor(),\n",
    "    # Normalizes RGB\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])  # Imagenet mean/std\n",
    "])\n",
    "\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961f51ed",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1119737",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../dataset/split\"\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(os.path.join(data_dir, \"train\"), transform=train_transforms)\n",
    "val_dataset = datasets.ImageFolder(os.path.join(data_dir, \"val\"), transform=val_test_transforms)\n",
    "test_dataset = datasets.ImageFolder(os.path.join(data_dir, \"test\"), transform=val_test_transforms)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class_names = train_dataset.classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61c698e",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b316748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plastic, Metal, Cardboard, Glass, Trash\n",
    "target_classes = 5\n",
    "\n",
    "# Load pretrained MobileNetV2 model (pretrained on ImageNet)\n",
    "model = models.mobilenet_v2(pretrained=True)\n",
    "# Replace final layer with custom classifier, how we adapt pretrained model to task.\n",
    "model.classifier[1] = nn.Linear(model.last_channel, target_classes)\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e24653",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b28f571",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7282bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrossEntropyLoss for multi-class classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f76a43",
   "metadata": {},
   "source": [
    "### Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cd4e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 116/221 [13:50<12:34,  7.18s/it]"
     ]
    }
   ],
   "source": [
    "# Set batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Loop for training\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in tqdm(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Zero gradients from previous step\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track metrics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * correct / total\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}% \"\n",
    "          f\"| Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
