{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6333ed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21dee7b",
   "metadata": {},
   "source": [
    "## Define Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf036f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 224\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    # Resizes images to 224x224, consider using CenterCrop or pad is aspect ratio wanted to be preserved\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    # Randomly flips the image horizontally for data augmentation, improves generalization\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # Converts image to PyTorch tensor\n",
    "    transforms.ToTensor(),\n",
    "    # Normalizes RGB\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])  # Imagenet mean/std\n",
    "])\n",
    "\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9e3bac",
   "metadata": {},
   "source": [
    "To prepare the images for training a lightweight pretrained CNN (like MobileNetV2), we apply the following transformation pipelines using `torchvision.transforms`:\n",
    "\n",
    "### üîÅ Training Transforms:\n",
    "- Scales all images to `224x224` pixels to match the expected input size of most pretrained models.  \n",
    "  *(Note: Use `CenterCrop` or `Pad` instead if you want to preserve aspect ratio.)*\n",
    "- *Randomly flips images horizontally*: This is a simple data augmentation technique that improves generalization by introducing more variation into the training set.\n",
    "- *Converts a PIL image (H x W x C) to a PyTorch tensor (C x H x W), and scales pixel values to the range [0.0, 1.0].\n",
    "- *Applies channel-wise normalization using ImageNet's mean and standard deviation*: Ensures pixel distributions align with model expectations.\n",
    "\n",
    "### üìè Validation & Test Transforms:\n",
    "Same as training transforms **without** data augmentation. These transformations ensure evaluation is consistent and reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9490613a",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1119737",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../dataset/split\"\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(os.path.join(data_dir, \"train\"), transform=train_transforms)\n",
    "val_dataset = datasets.ImageFolder(os.path.join(data_dir, \"val\"), transform=val_test_transforms)\n",
    "test_dataset = datasets.ImageFolder(os.path.join(data_dir, \"test\"), transform=val_test_transforms)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class_names = train_dataset.classes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
