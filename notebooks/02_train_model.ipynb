{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6333ed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21dee7b",
   "metadata": {},
   "source": [
    "## Define Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf036f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 224\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    # Resizes images to 224x224, consider using CenterCrop or pad is aspect ratio wanted to be preserved\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    # Randomly flips the image horizontally for data augmentation, improves generalization\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # Converts image to PyTorch tensor\n",
    "    transforms.ToTensor(),\n",
    "    # Normalizes RGB\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])  # Imagenet mean/std\n",
    "])\n",
    "\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961f51ed",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1119737",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../dataset/split\"\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(os.path.join(data_dir, \"train\"), transform=train_transforms)\n",
    "val_dataset = datasets.ImageFolder(os.path.join(data_dir, \"val\"), transform=val_test_transforms)\n",
    "test_dataset = datasets.ImageFolder(os.path.join(data_dir, \"test\"), transform=val_test_transforms)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class_names = train_dataset.classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61c698e",
   "metadata": {},
   "source": [
    "## Define Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b316748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/israelavendanojr./Desktop/project-repos/recycling-classifier/venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/israelavendanojr./Desktop/project-repos/recycling-classifier/venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "0.9%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /Users/israelavendanojr./.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "# Plastic, Metal, Cardboard, Glass, Trash\n",
    "target_classes = 5\n",
    "\n",
    "# Load pretrained MobileNetV2 model (pretrained on ImageNet)\n",
    "model = models.mobilenet_v2(pretrained=True)\n",
    "# Replace final layer with custom classifier, this is how we adapt pretrained model to task.\n",
    "model.classifier[1] = nn.Linear(model.last_channel, target_classes)\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
